from google.colab import drive
drive.mount('/content/drive')

!unzip -q /content/drive/MyDrive/waste_management_dataset.zip -d /content/dataset
import os
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

# -----------------------------
# Config
# -----------------------------
DATA_DIR = "/content/dataset/TrashType_Image_Dataset"
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
SEED = 123

# -----------------------------
# Inspect directory (important)
# -----------------------------
print("Classes (folders) found:")
for d in sorted(os.listdir(DATA_DIR)):
    if os.path.isdir(os.path.join(DATA_DIR, d)):
        print("  -", d)

# -----------------------------
# Load dataset (single root dir)
# -----------------------------
train_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="int",
    validation_split=0.2,
    subset="training",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="int",
    validation_split=0.2,
    subset="validation",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True
)

class_names = train_ds.class_names
NUM_CLASSES = len(class_names)
print("Class order:", class_names)

# Quick sanity check on labels
for images, labels in train_ds.take(1):
    print("Sample labels in first batch:", labels.numpy())
    break

# -----------------------------
# Performance optimizations
# -----------------------------
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)

# -----------------------------
# Data augmentation
# -----------------------------
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.15),
    layers.RandomContrast(0.2),
], name="data_augmentation")

# -----------------------------
# Base model
# -----------------------------
base_model = tf.keras.applications.MobileNetV2(
    input_shape=IMG_SIZE + (3,),
    include_top=False,
    weights="imagenet"
)

# -----------------------------
# Build model
# -----------------------------
inputs = tf.keras.Input(shape=IMG_SIZE + (3,))

x = data_augmentation(inputs)
x = layers.Rescaling(1./127.5, offset=-1)(x)   # <-- FIX
x = base_model(x, training=False)

x = layers.GlobalAveragePooling2D()(x)
x = layers.BatchNormalization()(x)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.4)(x)
outputs = layers.Dense(NUM_CLASSES, activation="softmax")(x)

model = models.Model(inputs, outputs, name="waste_classifier")
model.summary()

# ======================================================
# PHASE 1 â€” TRAIN CLASSIFIER HEAD
# ======================================================
base_model.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

print("ðŸ”µ Phase 1: Training classifier head")
history1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=1
)

# ======================================================
# PHASE 2 â€” FINE-TUNE TOP LAYERS
# ======================================================
base_model.trainable = True

# unfreeze only last 20â€“30 layers
for layer in base_model.layers[:-25]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

print("ðŸŸ¢ Phase 2: Fine-tuning top layers")
history2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=1
)

# ======================================================
# SAVE MODEL AND CLASS NAMES
# ======================================================
model.save("waste_classifier_model.keras")
np.save("waste_class_names.npy", np.array(class_names))
print("âœ… Final model saved at waste_classifier_model.keras")
print("âœ… Class names saved at waste_class_names.npy")
